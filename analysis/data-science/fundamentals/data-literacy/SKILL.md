---
name: "データリテラシー"
description: "データの型、欠損値の扱い方、データ品質の理解などデータ分析の基礎知識"
---

# データリテラシー: データを読み解く基礎力

## このスキルを使う場面

- データセットを初めて扱うとき
- データの品質を評価したい
- 欠損値や異常値の対処が必要
- データ型に応じた処理を選択したい
- データの信頼性を判断したい
- 分析の前処理方針を決定したい

## 思考プロセス

### フェーズ1: データ型の理解

**ステップ1: 数値型データ**

**1. 連続値（Continuous）**

- [ ] 小数点を含む実数
- [ ] 例：身長、体重、温度、売上金額
- [ ] 無限の値を取りうる
- [ ] 平均・標準偏差が意味を持つ

**適用可能な分析:**
- 回帰分析
- 相関分析
- 時系列分析
- 正規化・標準化

**2. 離散値（Discrete）**

- [ ] 整数値
- [ ] 例：人数、回数、商品個数
- [ ] 有限または可算個の値
- [ ] カウントデータ

**適用可能な分析:**
- ポアソン分布
- 二項分布
- カウントデータモデル

**3. 比率・割合**

- [ ] 0から1の範囲
- [ ] 例：合格率、シェア率、確率
- [ ] 特殊な処理が必要な場合あり

**ステップ2: カテゴリ型データ**

**1. 名義尺度（Nominal）**

- [ ] 順序に意味がない
- [ ] 例：性別、都道府県、商品カテゴリ
- [ ] 最頻値のみ意味を持つ
- [ ] ワンホットエンコーディング

**処理方法:**
- ダミー変数化
- ラベルエンコーディング（順序に意味がない場合は注意）
- ターゲットエンコーディング

**2. 順序尺度（Ordinal）**

- [ ] 順序に意味がある
- [ ] 例：満足度（低・中・高）、学年、評価ランク
- [ ] 中央値が意味を持つ
- [ ] 順序を保持したエンコーディング

**処理方法:**
- 順序を保持した数値化
- 順序ロジスティック回帰
- スコアリング

**3. 二値データ（Binary）**

- [ ] 2つの値のみ
- [ ] 例：Yes/No、True/False、合格/不合格
- [ ] ベルヌーイ分布
- [ ] 0/1にエンコード

**適用可能な分析:**
- ロジスティック回帰
- カイ二乗検定
- 分類モデル

**ステップ3: 時間・日付データ**

**1. タイムスタンプ**

- [ ] 特定の時点
- [ ] 例：2025-01-15 14:30:00
- [ ] 時系列分析の基礎
- [ ] タイムゾーンに注意

**2. 期間データ**

- [ ] 時間の長さ
- [ ] 例：3日間、2時間30分
- [ ] 集計・サマリーに使用

**3. 周期性**

- [ ] 曜日、月、季節
- [ ] 周期的パターンの特徴量化
- [ ] サイクリカルエンコーディング

**移行条件:**

- [ ] データ型を正しく識別できた
- [ ] 各データ型に適した処理を理解した
- [ ] データ型変換の必要性を判断できた

### フェーズ2: 欠損値の理解と対処

**ステップ1: 欠損値のメカニズム**

**1. MCAR（Missing Completely At Random）**

- [ ] 完全にランダムな欠損
- [ ] 他のデータと無関係
- [ ] バイアスを生まない
- [ ] 単純削除も可能

**2. MAR（Missing At Random）**

- [ ] 観測された変数に依存
- [ ] 例：高齢者の収入データが欠損しやすい
- [ ] 適切な補完が必要
- [ ] 多重代入法が有効

**3. MNAR（Missing Not At Random）**

- [ ] 欠損自体に意味がある
- [ ] 例：高収入の人が収入を回答しない
- [ ] 最も扱いが難しい
- [ ] 慎重な分析設計が必要

**ステップ2: 欠損値の対処方法**

**1. 削除**

**リストワイズ削除:**
- [ ] 欠損を含む行全体を削除
- [ ] データ量が十分な場合のみ
- [ ] MCARの場合に有効

**ペアワイズ削除:**
- [ ] 分析ごとに使用可能なデータを使用
- [ ] データ量を最大化
- [ ] 結果の一貫性に注意

**2. 補完（Imputation）**

**平均値補完:**
- [ ] 数値データに使用
- [ ] シンプルだが分散を過小評価
- [ ] 外れ値に影響されやすい

**中央値補完:**
- [ ] 外れ値に頑健
- [ ] 偏った分布に適する

**最頻値補完:**
- [ ] カテゴリデータに使用
- [ ] 最も出現頻度の高い値で補完

**予測モデル補完:**
- [ ] 他の変数から予測
- [ ] 線形回帰、k-NN、ランダムフォレスト
- [ ] より精度の高い補完

**多重代入法（Multiple Imputation）:**
- [ ] 複数の補完データセットを生成
- [ ] 不確実性を考慮
- [ ] 統計的に最も適切

**3. 欠損フラグの追加**

- [ ] 欠損したことを新しい変数として記録
- [ ] 欠損パターン自体に情報がある場合
- [ ] MNARの場合に有効

**ステップ3: 欠損値対処の判断基準**

**欠損率で判断:**
- [ ] < 5%: 削除または単純補完
- [ ] 5-20%: 慎重な補完
- [ ] > 20%: 変数の除外を検討

**データの重要度:**
- [ ] 重要な変数: 高度な補完
- [ ] 補助的な変数: 削除を検討

**分析の目的:**
- [ ] 予測モデル: 予測補完
- [ ] 統計的推論: 多重代入法
- [ ] 探索的分析: 単純補完

**移行条件:**

- [ ] 欠損値のメカニズムを理解した
- [ ] 適切な対処方法を選択できた
- [ ] 対処の影響を評価した

### フェーズ3: 異常値・外れ値の検出と対処

**ステップ1: 異常値の検出**

**1. 統計的手法**

**IQR（四分位範囲）法:**
```
Q1 - 1.5 × IQR < 正常値 < Q3 + 1.5 × IQR
```
- [ ] 分布を仮定しない
- [ ] 頑健な方法
- [ ] 箱ひげ図で視覚化

**Z-スコア法:**
```
|Z| = |(x - μ) / σ| > 3
```
- [ ] 正規分布を仮定
- [ ] 標準偏差の倍数で判定
- [ ] シンプルで理解しやすい

**2. 視覚的検出**

- [ ] 散布図での確認
- [ ] ヒストグラムでの分布確認
- [ ] 箱ひげ図
- [ ] Q-Qプロット

**3. ドメイン知識による判定**

- [ ] 物理的にありえない値（負の年齢など）
- [ ] ビジネスルールに反する値
- [ ] 明らかな入力エラー

**ステップ2: 異常値の対処**

**1. 削除**

- [ ] 明らかなエラーの場合
- [ ] データ量が十分な場合
- [ ] 削除の理由を記録

**2. 補正**

**キャッピング（Winsorizing）:**
- [ ] 上下限値で切り詰める
- [ ] 外れ値の影響を軽減
- [ ] 情報は残す

**変換:**
- [ ] 対数変換
- [ ] 平方根変換
- [ ] Box-Cox変換

**3. 保持**

- [ ] 真の異常値の場合
- [ ] 重要な情報を含む場合
- [ ] 異常検知が目的の場合

**移行条件:**

- [ ] 異常値を検出した
- [ ] 異常値の原因を調査した
- [ ] 適切な対処方法を選択した

### フェーズ4: データ品質の評価

**ステップ1: データ品質の次元**

**1. 正確性（Accuracy）**

- [ ] データが真の値を反映しているか
- [ ] 測定誤差の評価
- [ ] ソースの信頼性確認

**2. 完全性（Completeness）**

- [ ] 必要なデータがすべて揃っているか
- [ ] 欠損率の確認
- [ ] カバレッジの評価

**3. 一貫性（Consistency）**

- [ ] データ間で矛盾がないか
- [ ] 整合性ルールの検証
- [ ] 重複の確認

**4. 適時性（Timeliness）**

- [ ] データが最新か
- [ ] 更新頻度は十分か
- [ ] 鮮度の確認

**5. 妥当性（Validity）**

- [ ] データ形式が正しいか
- [ ] ビジネスルールに準拠しているか
- [ ] 範囲チェック

**ステップ2: データプロファイリング**

**1. 単変量プロファイリング**

- [ ] 各列の基本統計量
- [ ] データ型と値の範囲
- [ ] ユニーク値の数
- [ ] 欠損値の割合

**2. 多変量プロファイリング**

- [ ] 変数間の関係性
- [ ] 依存関係の確認
- [ ] 整合性チェック

**3. パターン分析**

- [ ] 時系列パターン
- [ ] 周期性の確認
- [ ] 異常なパターンの検出

**移行条件:**

- [ ] データ品質を多角的に評価した
- [ ] 品質問題を特定した
- [ ] 改善の優先順位を決定した

## 判断のポイント

### 欠損値対処の選択

**削除すべき場合:**
- 欠損率が非常に低い（< 5%）
- MCAR が成り立つ
- データ量が十分

**補完すべき場合:**
- 欠損率が中程度（5-20%）
- 重要な変数
- MAR が成り立つ

**変数を除外すべき場合:**
- 欠損率が非常に高い（> 50%）
- 補完の信頼性が低い
- 重要度が低い変数

### 異常値の対処

**削除すべき場合:**
- 明らかなデータエラー
- 物理的にありえない値
- 分析に悪影響

**保持すべき場合:**
- 真の異常値
- 異常検知が目的
- 重要な情報を含む

**変換すべき場合:**
- 分布の歪みが大きい
- 外れ値の影響を軽減したい
- モデルの仮定を満たすため

## よくある落とし穴

1. **データ型の誤認識**
   - ❌ 数値として保存されたカテゴリデータ
   - ✅ 意味を考えて正しく認識

2. **安易な削除**
   - ❌ 欠損を含むデータを無条件に削除
   - ✅ 欠損のメカニズムを理解

3. **不適切な補完**
   - ❌ すべて平均値で補完
   - ✅ データの特性に応じた補完

4. **外れ値の無視**
   - ❌ 外れ値を見て見ぬふり
   - ✅ 原因を調査し適切に対処

5. **データ品質の過信**
   - ❌ 提供されたデータを盲信
   - ✅ 必ず品質チェック

## 検証ポイント

### データ型の確認

- [ ] すべての列のデータ型を確認した
- [ ] 誤った型を修正した
- [ ] 適切なエンコーディングを選択した

### 欠損値の処理

- [ ] 欠損値の存在を確認した
- [ ] 欠損のメカニズムを考察した
- [ ] 適切な対処方法を実施した
- [ ] 対処の影響を評価した

### 異常値の処理

- [ ] 異常値を検出した
- [ ] 原因を調査した
- [ ] 適切に対処した
- [ ] 処理の記録を残した

### データ品質の評価

- [ ] 品質の各次元を評価した
- [ ] 問題点を特定した
- [ ] 改善計画を立てた

## 他スキルとの連携

### data-literacy → data-preprocessing

リテラシーから前処理へ：

1. このスキルでデータを理解
2. 品質問題を特定
3. data-preprocessing で処理実施
4. 処理後のデータを再評価

### data-literacy → exploratory-data-analysis

理解してから探索：

1. このスキルでデータの基本を把握
2. 適切なデータ型に変換
3. EDA で詳細な分析
4. パターンと洞察を発見

## データリテラシーのベストプラクティス

### ドキュメント化

- データディクショナリの作成
- 列の意味と型を記録
- 欠損・異常値の対処履歴
- ビジネスルールの文書化

### 早期チェック

- 分析開始前の品質確認
- 自動化されたチェック
- 継続的モニタリング
- アラート設定

### コミュニケーション

- データ提供者との対話
- ドメインエキスパートへの相談
- 仮定の明示
- 制約の共有
